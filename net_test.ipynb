{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 151 (1127346361.py, line 154)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 154\u001b[1;36m\u001b[0m\n\u001b[1;33m    for m in self.modules():\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 151\n"
     ]
    }
   ],
   "source": [
    "class LinearResNetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearResNetBlock, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 64\n",
    "        self.block_layer_nums =3\n",
    "            \n",
    "        # Define layers for the function f (MLP)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.block_layer_nums - 1):  # -2 because we already added one layer and last layer is already defined\n",
    "            self.layers.append(nn.Linear(self.hidden_dim,self.hidden_dim ))\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorms = nn.ModuleList()\n",
    "        for _ in range(self.block_layer_nums - 1):  # -1 because layer normalization is not applied to the last layer\n",
    "            self.layernorms.append(nn.LayerNorm(self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the function f (MLP)\n",
    "        out = x\n",
    "        for i in range(self.block_layer_nums - 1):  # -1 because last layer is already applied outside the loop\n",
    "            out = self.layers[i](out)\n",
    "            out = self.layernorms[i](out)\n",
    "            out = torch.relu(out)\n",
    "        \n",
    "        # Element-wise addition of input x and output of function f(x)\n",
    "        out = x + out\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 定义残差网络18/34的残差结构，为2个3*3的卷积\n",
    "class BasicBlock(nn.Module):\n",
    "    # 判断残差结构中，主分支的卷积核个数是否发生变化，不变则为1\n",
    "    expansion = 1\n",
    "    def __init__(self,in_channel,out_channel,stride=1,downsample=None,**kwargs):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=out_channel,\n",
    "                               kernel_size=3,stride=stride,padding=1,bias=False\n",
    "        )\n",
    "        # 使用批量归一化\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel,out_channels=out_channel,\n",
    "                               kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample =downsample\n",
    "    def forward(self,x):\n",
    " \n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity=self.downsample(x)  # self当前对象实例，用于访问对象的属性和方法\n",
    "        out = self.conv1(x)\n",
    "        out =self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out +=identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    # expansion是指在每个小残差块内，减少尺度增加维度的倍数\n",
    "    # 该类输出的通道是输入的四倍\n",
    "    expansion = 4\n",
    "    def __init__(self,in_channel,out_channel,strdie=1,downsample=None,\n",
    "                 groups=1,width_per_group=64):\n",
    "        super(Bottleneck,self).__init__()\n",
    " \n",
    "        width = int(out_channel*(width_per_group/64.))*groups\n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=width,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2= nn.Conv2d(in_channels=width,out_channels=width,groups=groups\n",
    "                              ,kernel_size=3,stride=strdie,bias=False,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    " \n",
    "        self.conv3 = nn.Conv2d(in_channels=width,out_channels=width,groups=groups,\n",
    "                               kernel_size=3,stride=strdie,bias=False,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansio)\n",
    " \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "    def forward(self,x):\n",
    "        identify = x\n",
    "        if self.downsample is not None:\n",
    "            identify = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    " \n",
    "        out+=identify\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "#定义ResNet类\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block_type,blocks_num,linear = False,include_top=False,groups=1,width_per_group=64):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.linear = linear\n",
    "        # maxpool的输出通道数为8，残差结构的输入通道为8\n",
    "        self.origin_chanel = 1\n",
    "        self.in_channel = 8\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "        self.conv1 = nn.Conv2d(self.origin_chanel,self.in_channel,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,1,kernel_size=1)\n",
    "        # 浅层步长为1，深层的步长为2\n",
    "        # block：定义了两种残差模块\n",
    "        # block_num:定义了残差快的个数\n",
    "        self.layer1 = self._make_layer(block_type,8,blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block_type, 8, blocks_num[1],stride=2)\n",
    "        self.layer3 = self._make_layer(block_type, 16, blocks_num[2],stride=2)\n",
    "        self.layer4 = self._make_layer(block_type, 16, blocks_num[3],stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64,64)\n",
    "\n",
    "        if self.linear == True :\n",
    "            self.linearResnet = LinearResNetBlock()\n",
    "        # 遍历网络中的每一层\n",
    "        # 继承nn.Moudle类中的一个方法：self.Moudle(),它会返回该网络中所有的moudles\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                # kaiming正态分布初始化，使得卷积层反向传播的输出的方差都为1\n",
    "                # fan_in权重是通过线性层隐形确定\n",
    "                # fan_out：通过创建随机矩阵显式创建权重\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
    "    def _make_layer(self,block_type,channel,block_num,stride=1):\n",
    "        downsample = None\n",
    "        if stride !=1 or self.in_channel !=channel*block_type.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel,channel*block_type.expansion,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(channel*block_type.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block_type(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group = self.width_per_group))\n",
    "        self.in_channel = channel*block_type.expansion\n",
    " \n",
    "        for _ in range(1,block_num):\n",
    "            layers.append(block_type(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group = self.width_per_group))\n",
    "        # Sequential:自定义顺序连接成模型，生成网络结构\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # 上述都是静态层，下边是动态层\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.linear == True:\n",
    "            x = torch.flatten(x,1)\n",
    "            x = self.linearResnet(x)\n",
    "        \n",
    "        return x\n",
    " \n",
    " \n",
    "# ResNet()中block_type参数对应的位置是BasicBlock或Bottleneck\n",
    "# ResNet()中blocks_num[0-3]对应[3, 4, 6, 3]，表示残差模块中的残差数\n",
    "# 34层的resnet\n",
    "def resnet34():\n",
    "    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    " \n",
    " \n",
    "# 50层的resnet\n",
    "def resnet50(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    " \n",
    " \n",
    "# 101层的resnet\n",
    "def resnet101(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
