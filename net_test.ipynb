{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import keyboard\n",
    "import sys\n",
    "sys.path.append(\"model\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "X,y = torch.load('data_fake.pth')  #输入数据\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 数据预处理 标准化数据\n",
    "\n",
    "# 划分训练集、验证集和测试集\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=20)\n",
    "\n",
    "# 将数据转移到 GPU（如果可用\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "X_train, X_val, X_test = X_train.to(device), X_val.to(device), X_test.to(device)\n",
    "y_train, y_val, y_test = y_train.to(device), y_val.to(device), y_test.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearResNetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearResNetBlock, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = 64\n",
    "        self.block_layer_nums =3\n",
    "            \n",
    "        # Define layers for the function f (MLP)\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(self.block_layer_nums - 1):  # -2 because we already added one layer and last layer is already defined\n",
    "            self.layers.append(nn.Linear(self.hidden_dim,self.hidden_dim ))\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorms = nn.ModuleList()\n",
    "        for _ in range(self.block_layer_nums - 1):  # -1 because layer normalization is not applied to the last layer\n",
    "            self.layernorms.append(nn.LayerNorm(self.hidden_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the function f (MLP)\n",
    "        out = x\n",
    "        for i in range(self.block_layer_nums - 1):  # -1 because last layer is already applied outside the loop\n",
    "            out = self.layers[i](out)\n",
    "            out = self.layernorms[i](out)\n",
    "            out = torch.relu(out)\n",
    "        \n",
    "        # Element-wise addition of input x and output of function f(x)\n",
    "        out = x + out\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 定义残差网络18/34的残差结构，为2个3*3的卷积\n",
    "class BasicBlock(nn.Module):\n",
    "    # 判断残差结构中，主分支的卷积核个数是否发生变化，不变则为1\n",
    "    expansion = 1\n",
    "    def __init__(self,in_channel,out_channel,stride=1,downsample=None,**kwargs):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=out_channel,\n",
    "                               kernel_size=3,stride=stride,padding=1,bias=False\n",
    "        )\n",
    "        # 使用批量归一化\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel,out_channels=out_channel,\n",
    "                               kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample =downsample\n",
    "    def forward(self,x):\n",
    " \n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity=self.downsample(x)  # self当前对象实例，用于访问对象的属性和方法\n",
    "        out = self.conv1(x)\n",
    "        out =self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out +=identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class Bottleneck(nn.Module):\n",
    "    # expansion是指在每个小残差块内，减少尺度增加维度的倍数\n",
    "    # 该类输出的通道是输入的四倍\n",
    "    expansion = 4\n",
    "    def __init__(self,in_channel,out_channel,strdie=1,downsample=None,\n",
    "                 groups=1,width_per_group=64):\n",
    "        super(Bottleneck,self).__init__()\n",
    " \n",
    "        width = int(out_channel*(width_per_group/64.))*groups\n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel,out_channels=width,kernel_size=1,stride=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        self.conv2= nn.Conv2d(in_channels=width,out_channels=width,groups=groups\n",
    "                              ,kernel_size=3,stride=strdie,bias=False,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    " \n",
    "        self.conv3 = nn.Conv2d(in_channels=width,out_channels=width,groups=groups,\n",
    "                               kernel_size=3,stride=strdie,bias=False,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansio)\n",
    " \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "    def forward(self,x):\n",
    "        identify = x\n",
    "        if self.downsample is not None:\n",
    "            identify = self.downsample(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    " \n",
    "        out+=identify\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "#定义ResNet类\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,block_type,blocks_num,linear = False,include_top=False,groups=1,width_per_group=64):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.linear = linear\n",
    "        # maxpool的输出通道数为8，残差结构的输入通道为8\n",
    "        self.origin_chanel = 1\n",
    "        self.in_channel = 8\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "        self.conv1 = nn.Conv2d(self.origin_chanel,self.in_channel,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        self.conv2 = nn.Conv2d(16,1,kernel_size=1)\n",
    "        # 浅层步长为1，深层的步长为2\n",
    "        # block：定义了两种残差模块\n",
    "        # block_num:定义了残差快的个数\n",
    "        self.layer1 = self._make_layer(block_type,8,blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block_type, 8, blocks_num[1],stride=2)\n",
    "        self.layer3 = self._make_layer(block_type, 16, blocks_num[2],stride=2)\n",
    "        self.layer4 = self._make_layer(block_type, 16, blocks_num[3],stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64,64)\n",
    "\n",
    "        if self.linear == True :\n",
    "            self.linearResnet = LinearResNetBlock()\n",
    "        # 遍历网络中的每一层\n",
    "        # 继承nn.Moudle类中的一个方法：self.Moudle(),它会返回该网络中所有的moudles\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                # kaiming正态分布初始化，使得卷积层反向传播的输出的方差都为1\n",
    "                # fan_in权重是通过线性层隐形确定\n",
    "                # fan_out：通过创建随机矩阵显式创建权重\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
    "    def _make_layer(self,block_type,channel,block_num,stride=1):\n",
    "        downsample = None\n",
    "        if stride !=1 or self.in_channel !=channel*block_type.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel,channel*block_type.expansion,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(channel*block_type.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block_type(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group = self.width_per_group))\n",
    "        self.in_channel = channel*block_type.expansion\n",
    " \n",
    "        for _ in range(1,block_num):\n",
    "            layers.append(block_type(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group = self.width_per_group))\n",
    "        # Sequential:自定义顺序连接成模型，生成网络结构\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # 上述都是静态层，下边是动态层\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.linear == True:\n",
    "            x = torch.flatten(x,1)\n",
    "            x = self.linearResnet(x)\n",
    "        \n",
    "        return x\n",
    " \n",
    " \n",
    "# ResNet()中block_type参数对应的位置是BasicBlock或Bottleneck\n",
    "# ResNet()中blocks_num[0-3]对应[3, 4, 6, 3]，表示残差模块中的残差数\n",
    "# 34层的resnet\n",
    "def resnet34():\n",
    "    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    " \n",
    " \n",
    "# 50层的resnet\n",
    "def resnet50(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    " \n",
    " \n",
    "# 101层的resnet\n",
    "def resnet101(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = resnet34().to(device)\n",
    "\n",
    "loss_function=nn.MSELoss()\n",
    "\n",
    "def criterion(output,label,mode = 1):\n",
    "    if mode == 1:\n",
    "        return loss_function(output,label)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# 准备 DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "patience_counter = 0\n",
    "patience_on  = 0\n",
    "patience = 9\n",
    "stop_training = 0\n",
    "\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 热键函数\n",
    "def on_press(key):\n",
    "    global patience_on\n",
    "    global stop_training\n",
    "    if key.name == 's':#终止训练，储存最好模型\n",
    "        print(\"Training stopped. Saving current best model...\")\n",
    "        print(f'best validation loss : {best_val_loss/ len(val_loader)}')\n",
    "        best_model = model\n",
    "        best_model.load_state_dict(torch.load('model\\Temporary_Model\\model_best.pth'))\n",
    "        # 保存效果最好的模型\n",
    "        torch.save(best_model.state_dict(), 'model\\Temporary_Model\\model_best.pth')\n",
    "        stop_training = 1\n",
    "\n",
    "    if key.name == 'q': #中途储存当前最好模型，但并不终止训练\n",
    "        print(\"Saving current best model to pause1...\")\n",
    "        print(f'best validation loss : {best_val_loss/ len(val_loader)}')\n",
    "        best_model = model\n",
    "        best_model.load_state_dict(torch.load('model\\Temporary_Model\\model_best.pth'))\n",
    "        # 保存效果最好的模型\n",
    "        torch.save(best_model.state_dict(), 'model\\Temporary_Model\\model_pause1.pth')\n",
    "        \n",
    "    if key.name == 'w': #中途储存当前最好模型，但并不终止训练\n",
    "        print(\"Saving current best model to pause2...\")\n",
    "        print(f'best validation loss : {best_val_loss/ len(val_loader)}')\n",
    "        best_model = model\n",
    "        best_model.load_state_dict(torch.load('model\\Temporary_Model\\model_best.pth'))\n",
    "        # 保存效果最好的模型\n",
    "        torch.save(best_model.state_dict(), 'model\\Temporary_Model\\model_pause2.pth')\n",
    "\n",
    "    if key.name == 'e': #中途储存当前最好模型，但并不终止训练\n",
    "        print(\"Saving current best model to pause3...\")\n",
    "        print(f'best validation loss : {best_val_loss/ len(val_loader)}')\n",
    "        best_model = model\n",
    "        best_model.load_state_dict(torch.load('model\\Temporary_Model\\model_best.pth'))\n",
    "        # 保存效果最好的模型\n",
    "        torch.save(best_model.state_dict(), 'model\\Temporary_Model\\model_pause3.pth')\n",
    "\n",
    "    if key.name == 'o': #开启early stopping\n",
    "        patience_on  = 1\n",
    "        print(\"early stopping is turned on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.12760910334495398, Validation Loss: 0.18723857402801514, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.18723857402801514\n",
      "Epoch 2, Training Loss: 0.09268295478362304, Validation Loss: 0.174066424369812, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.174066424369812\n",
      "Epoch 3, Training Loss: 0.07183954664147817, Validation Loss: 0.1585356593132019, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1585356593132019\n",
      "Epoch 4, Training Loss: 0.06086513629326454, Validation Loss: 0.1503712683916092, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1503712683916092\n",
      "Epoch 5, Training Loss: 0.052030253009154245, Validation Loss: 0.14642742276191711, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.14642742276191711\n",
      "Epoch 6, Training Loss: 0.044976111788016096, Validation Loss: 0.13814420998096466, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.13814420998096466\n",
      "Epoch 7, Training Loss: 0.04056399258283468, Validation Loss: 0.13723623752593994, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.13723623752593994\n",
      "Epoch 8, Training Loss: 0.03687643804229223, Validation Loss: 0.1321408748626709, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1321408748626709\n",
      "Epoch 9, Training Loss: 0.03343954696678198, Validation Loss: 0.13043712079524994, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.13043712079524994\n",
      "Epoch 10, Training Loss: 0.0317058594754109, Validation Loss: 0.1274956315755844, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1274956315755844\n",
      "Epoch 11, Training Loss: 0.029034654394938395, Validation Loss: 0.1277468502521515, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.1274956315755844\n",
      "Epoch 12, Training Loss: 0.02707038762477728, Validation Loss: 0.12294785678386688, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.12294785678386688\n",
      "Epoch 13, Training Loss: 0.025899386749817774, Validation Loss: 0.12194917351007462, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.12194917351007462\n",
      "Epoch 14, Training Loss: 0.024389123400816552, Validation Loss: 0.1215478777885437, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1215478777885437\n",
      "Epoch 15, Training Loss: 0.023185989461266078, Validation Loss: 0.1212129294872284, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1212129294872284\n",
      "Epoch 16, Training Loss: 0.022097754936951857, Validation Loss: 0.11843021959066391, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11843021959066391\n",
      "Epoch 17, Training Loss: 0.020896601848877393, Validation Loss: 0.11626788973808289, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11626788973808289\n",
      "Epoch 18, Training Loss: 0.01988770139332001, Validation Loss: 0.11569993942975998, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11569993942975998\n",
      "Epoch 19, Training Loss: 0.01901350113061758, Validation Loss: 0.11577294021844864, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.11569993942975998\n",
      "Epoch 20, Training Loss: 0.018907187291635916, Validation Loss: 0.11519533395767212, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11519533395767212\n",
      "Epoch 21, Training Loss: 0.017576104818055265, Validation Loss: 0.11487375199794769, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11487375199794769\n",
      "Epoch 22, Training Loss: 0.0170666381286887, Validation Loss: 0.11315847933292389, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11315847933292389\n",
      "Epoch 23, Training Loss: 0.016709595273893613, Validation Loss: 0.11361495405435562, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.11315847933292389\n",
      "Epoch 24, Training Loss: 0.016116574836465027, Validation Loss: 0.11274861544370651, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11274861544370651\n",
      "Epoch 25, Training Loss: 0.015780274254771378, Validation Loss: 0.11253508925437927, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11253508925437927\n",
      "Epoch 26, Training Loss: 0.015261444478080822, Validation Loss: 0.11224167793989182, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11224167793989182\n",
      "Epoch 27, Training Loss: 0.014624496706976341, Validation Loss: 0.11134666204452515, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11134666204452515\n",
      "Epoch 28, Training Loss: 0.014166411084051315, Validation Loss: 0.11045359820127487, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.11045359820127487\n",
      "Epoch 29, Training Loss: 0.014219000219152523, Validation Loss: 0.11199440807104111, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.11045359820127487\n",
      "Epoch 30, Training Loss: 0.013768455300193567, Validation Loss: 0.10907018929719925, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10907018929719925\n",
      "Epoch 31, Training Loss: 0.013327420116044007, Validation Loss: 0.11068957298994064, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10907018929719925\n",
      "Epoch 32, Training Loss: 0.013233665520182023, Validation Loss: 0.10873014479875565, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10873014479875565\n",
      "Epoch 33, Training Loss: 0.012873227206560282, Validation Loss: 0.10772372037172318, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10772372037172318\n",
      "Epoch 34, Training Loss: 0.013083227981741611, Validation Loss: 0.10804028064012527, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10772372037172318\n",
      "Epoch 35, Training Loss: 0.01230411439274366, Validation Loss: 0.10668566823005676, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10668566823005676\n",
      "Epoch 36, Training Loss: 0.012028697090080151, Validation Loss: 0.10522596538066864, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10522596538066864\n",
      "Epoch 37, Training Loss: 0.011279657698021485, Validation Loss: 0.10630203038454056, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10522596538066864\n",
      "Epoch 38, Training Loss: 0.011461994467446437, Validation Loss: 0.10442885756492615, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10442885756492615\n",
      "Epoch 39, Training Loss: 0.010969620986053577, Validation Loss: 0.1034424751996994, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.1034424751996994\n",
      "Epoch 40, Training Loss: 0.011238347309140058, Validation Loss: 0.10334064811468124, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10334064811468124\n",
      "Epoch 41, Training Loss: 0.01096037023056012, Validation Loss: 0.10320811718702316, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10320811718702316\n",
      "Epoch 42, Training Loss: 0.01055040081533102, Validation Loss: 0.1054120808839798, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10320811718702316\n",
      "Epoch 43, Training Loss: 0.010275394082642518, Validation Loss: 0.10321243852376938, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.10320811718702316\n",
      "Epoch 44, Training Loss: 0.010184537261151351, Validation Loss: 0.1039392352104187, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.10320811718702316\n",
      "Epoch 45, Training Loss: 0.010413558150713261, Validation Loss: 0.10134822130203247, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 46, Training Loss: 0.01033226171365151, Validation Loss: 0.1022956445813179, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 47, Training Loss: 0.010297881009487005, Validation Loss: 0.10336223244667053, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 48, Training Loss: 0.009620601132225532, Validation Loss: 0.10270801186561584, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 49, Training Loss: 0.009808690502093388, Validation Loss: 0.10329194366931915, earlystopping is on, 4steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 50, Training Loss: 0.010138328187167645, Validation Loss: 0.10179629921913147, earlystopping is on, 5steps after last bestloss, best val-loss now : 0.10134822130203247\n",
      "Epoch 51, Training Loss: 0.010022699904556457, Validation Loss: 0.10060189664363861, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10060189664363861\n",
      "Epoch 52, Training Loss: 0.009801337638726601, Validation Loss: 0.10292908549308777, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10060189664363861\n",
      "Epoch 53, Training Loss: 0.009616144502965303, Validation Loss: 0.10047192126512527, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10047192126512527\n",
      "Epoch 54, Training Loss: 0.009335256145837216, Validation Loss: 0.10042402148246765, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.10042402148246765\n",
      "Epoch 55, Training Loss: 0.009138408869218368, Validation Loss: 0.10080710053443909, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.10042402148246765\n",
      "Epoch 56, Training Loss: 0.0090693891263352, Validation Loss: 0.098448246717453, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 57, Training Loss: 0.008815495404773034, Validation Loss: 0.09859111160039902, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 58, Training Loss: 0.008695419734487167, Validation Loss: 0.09968207776546478, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 59, Training Loss: 0.008866276699476518, Validation Loss: 0.09943690896034241, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 60, Training Loss: 0.009232959136939965, Validation Loss: 0.10147663950920105, earlystopping is on, 4steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 61, Training Loss: 0.009146729340920081, Validation Loss: 0.09947521984577179, earlystopping is on, 5steps after last bestloss, best val-loss now : 0.098448246717453\n",
      "Epoch 62, Training Loss: 0.008625073430056755, Validation Loss: 0.09710165113210678, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09710165113210678\n",
      "Epoch 63, Training Loss: 0.0088125798636331, Validation Loss: 0.09940355271100998, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.09710165113210678\n",
      "Epoch 64, Training Loss: 0.008628673600749327, Validation Loss: 0.09914880990982056, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.09710165113210678\n",
      "Epoch 65, Training Loss: 0.008685061385711798, Validation Loss: 0.09691394865512848, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 66, Training Loss: 0.008771571008345256, Validation Loss: 0.09764939546585083, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 67, Training Loss: 0.008305950878331294, Validation Loss: 0.09715855866670609, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 68, Training Loss: 0.00837428502451915, Validation Loss: 0.09771032631397247, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 69, Training Loss: 0.00838215033022257, Validation Loss: 0.09705576300621033, earlystopping is on, 4steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 70, Training Loss: 0.00816307832988409, Validation Loss: 0.09772596508264542, earlystopping is on, 5steps after last bestloss, best val-loss now : 0.09691394865512848\n",
      "Epoch 71, Training Loss: 0.008119253226770805, Validation Loss: 0.09666915982961655, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09666915982961655\n",
      "Epoch 72, Training Loss: 0.008055201480881525, Validation Loss: 0.09831389784812927, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.09666915982961655\n",
      "Epoch 73, Training Loss: 0.008005824262419572, Validation Loss: 0.09620782732963562, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09620782732963562\n",
      "Epoch 74, Training Loss: 0.007770683401479171, Validation Loss: 0.09437520802021027, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09437520802021027\n",
      "Epoch 75, Training Loss: 0.00804099508632834, Validation Loss: 0.09787876904010773, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.09437520802021027\n",
      "Epoch 76, Training Loss: 0.00847620663877863, Validation Loss: 0.0950307548046112, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.09437520802021027\n",
      "Epoch 77, Training Loss: 0.008353666438219639, Validation Loss: 0.0964449942111969, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.09437520802021027\n",
      "Epoch 78, Training Loss: 0.00839353703822081, Validation Loss: 0.09554871171712875, earlystopping is on, 4steps after last bestloss, best val-loss now : 0.09437520802021027\n",
      "Epoch 79, Training Loss: 0.008018415707808275, Validation Loss: 0.09207965433597565, earlystopping is on, 0steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 80, Training Loss: 0.007651647982689051, Validation Loss: 0.09347748756408691, earlystopping is on, 1steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 81, Training Loss: 0.007741993006605368, Validation Loss: 0.09539023041725159, earlystopping is on, 2steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 82, Training Loss: 0.0076930501426641755, Validation Loss: 0.09302711486816406, earlystopping is on, 3steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 83, Training Loss: 0.0075723603009604495, Validation Loss: 0.09418351948261261, earlystopping is on, 4steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 84, Training Loss: 0.007378308317409112, Validation Loss: 0.09432150423526764, earlystopping is on, 5steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 85, Training Loss: 0.007146707664315517, Validation Loss: 0.0941283255815506, earlystopping is on, 6steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 86, Training Loss: 0.007038375541854363, Validation Loss: 0.09476573765277863, earlystopping is on, 7steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "Epoch 87, Training Loss: 0.007398503116117074, Validation Loss: 0.09262273460626602, earlystopping is on, 8steps after last bestloss, best val-loss now : 0.09207965433597565\n",
      "在第88个epoch处，Validation loss did not improve for 9 epochs. Early stopping...\n",
      "best validation loss : 0.09207965433597565\n"
     ]
    }
   ],
   "source": [
    "keyboard.on_press(on_press)\n",
    "# 训练模型\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if stop_training:\n",
    "        break\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    loss1_total =0.0\n",
    "    loss2_total =0.0\n",
    "    loss3_total =0.0\n",
    "    loss4_total =0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss= criterion(outputs, labels, mode =1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += (loss/len(inputs)).item()\n",
    "\n",
    "    # 验证模型\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_outputs = model(val_inputs)\n",
    "\n",
    "            val_loss_single = criterion(val_outputs, val_labels, mode =1)\n",
    "            '''\n",
    "            loss1 = loss1/len(val_inputs)\n",
    "            loss2 = loss2/len(val_inputs)\n",
    "            loss3 = loss3/len(val_inputs)\n",
    "            loss4 = loss4/len(val_inputs)\n",
    "            '''\n",
    "            val_loss += val_loss_single/len(val_inputs)\n",
    "            '''\n",
    "            loss1_total += loss1\n",
    "            loss2_total += loss2\n",
    "            loss3_total += loss3\n",
    "            loss4_total += loss4\n",
    "            '''\n",
    "    #if val_loss / len(val_loader) <0.002:\n",
    "        #patience_on = 1\n",
    "    # Early Stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # 保存效果最好的模型\n",
    "        torch.save(model.state_dict(), 'model\\Temporary_Model\\model_best.pth')\n",
    "    else:\n",
    "        if patience_on == 1:\n",
    "            patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f'在第{epoch+1}个epoch处，Validation loss did not improve for {patience} epochs. Early stopping...')\n",
    "        print(f'best validation loss : {best_val_loss/ len(val_loader)}')\n",
    "        break\n",
    "    if patience_on == 0:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}, best val-loss now : {best_val_loss / len(val_loader)}')\n",
    "        #print(f'Validation Loss Part: A{loss1_total/ len(val_loader)}, B{loss2_total/ len(val_loader)}, C{loss3_total/ len(val_loader)}, D{loss4_total/ len(val_loader)}')\n",
    "    else: \n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}, earlystopping is on, {patience_counter}steps after last bestloss, best val-loss now : {best_val_loss / len(val_loader)}') \n",
    "\n",
    "\n",
    "keyboard.unhook_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3656, device='cuda:0')\n",
      "Test Loss: 2.1963865756988525\n"
     ]
    }
   ],
   "source": [
    "# 加载效果最好的模型\n",
    "best_model = model\n",
    "best_model.load_state_dict(torch.load('model\\Temporary_Model\\model_best.pth'))\n",
    "\n",
    "# 在测试集上评估模型\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1,shuffle = None)\n",
    "\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        \n",
    "        test_outputs = best_model(test_inputs)\n",
    "        if count == 0:\n",
    "            \n",
    "            count = 1\n",
    "        \n",
    "        test_loss_single= criterion(test_outputs, test_labels, mode =1)\n",
    "        test_loss += test_loss_single/len(test_inputs)\n",
    "\n",
    "        if test_loss_single>0.5 and count == 1:\n",
    "            count = 2 \n",
    "            print(test_loss_single)    \n",
    "        \n",
    "    print(f'Test Loss: {test_loss / len(test_loader)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
